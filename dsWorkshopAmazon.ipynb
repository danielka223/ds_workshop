{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "dsWorkshopAmazon.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/danielka223/ds_workshop/blob/master/dsWorkshopAmazon.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "LZukTBix5h8d",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<h1 align=\"left\">Helpfulness Prediction of Amazon Product Reviews</h1> \n",
        "<h2 align=\"left\">DS Workshop - Fall 18/19 - Tel Aviv University</h2> \n",
        "<h3 align=\"left\">By Daniel K.A, Ido Salomon, Itamar Mutzafi and Sagi Aharoni</h3> \n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "oChClvhQU2zT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "8u-UkcpMH7lU"
      },
      "cell_type": "markdown",
      "source": [
        "#Notebook Initialization\n",
        "This section should only run once on a new runtime"
      ]
    },
    {
      "metadata": {
        "id": "SAYW3dqpvhP7",
        "colab_type": "code",
        "cellView": "both",
        "outputId": "e147dd98-b149-4bbe-c165-64d1070ad8fa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "cell_type": "code",
      "source": [
        "#@title Install Required Packages\n",
        "\n",
        "!pip install wordcloud\n",
        "!pip install nltk"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: wordcloud in /usr/local/lib/python3.6/dist-packages (1.5.0)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.6/dist-packages (from wordcloud) (4.0.0)\n",
            "Requirement already satisfied: numpy>=1.6.1 in /usr/local/lib/python3.6/dist-packages (from wordcloud) (1.14.6)\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from pillow->wordcloud) (0.46)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.11.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "WDPJB0i_G5kJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title Import Python Libraries {display-mode: \"form\"}\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import pandas as pd\n",
        "import json\n",
        "import datetime\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "from contextlib import contextmanager\n",
        "from os.path import getsize, basename\n",
        "from tqdm import tqdm\n",
        "\n",
        "import requests\n",
        "import re\n",
        "\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import gzip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DWtoukAHx7F5",
        "colab_type": "code",
        "outputId": "a752e1b8-02b9-4fd5-c77d-e89fd1e9644a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "#@title Connect to Drive {display-mode: \"form\"}\n",
        "\n",
        "drive.mount('/content/drive/')\n",
        "            "
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "2zBOKxrkscj0",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title Infra Definitions\n",
        "\n",
        "@contextmanager\n",
        "def pbopen(filename, mode='r'):\n",
        "    total = getsize(filename)\n",
        "    pb = tqdm(total=total, unit=\"B\", unit_scale=True,\n",
        "              desc=basename(filename), miniters=1,\n",
        "              ncols=80, ascii=True)\n",
        "\n",
        "    def wrapped_line_iterator(fd):\n",
        "        processed_bytes = 0\n",
        "        for line in fd:\n",
        "            processed_bytes += len(line)\n",
        "            # update progress every MB.\n",
        "            if processed_bytes >= 1024 * 1024:\n",
        "                pb.update(processed_bytes)\n",
        "                processed_bytes = 0\n",
        "\n",
        "            yield line\n",
        "\n",
        "        # finally\n",
        "        pb.update(processed_bytes)\n",
        "        pb.close()\n",
        "\n",
        "    with open(filename, mode) as fd:\n",
        "        yield wrapped_line_iterator(fd)\n",
        "        \n",
        "#decompress input folder to output folder\n",
        "def ungzip(source_dir, dest_dir):\n",
        "  for src_name in glob.glob(os.path.join(source_dir, '*.gz')):\n",
        "      base = os.path.basename(src_name)\n",
        "      dest_name = os.path.join(dest_dir, base[:-3])\n",
        "      with gzip.open(src_name, 'rb') as infile:\n",
        "          with open(dest_name, 'wb') as outfile:\n",
        "              for line in infile:\n",
        "                  outfile.write(line)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ry95s1XyUwe4",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title Data Fetching\n",
        "\n",
        "# the following script downloads the Amazon Review Dataset into google drive\n",
        "\n",
        "root = \"drive/Team Drives/DS Workshop/data/\"\n",
        "compressed_path = root + \"compressed/\"\n",
        "\n",
        "#uncomment to download\n",
        "review_paths = [\n",
        "#                 \"https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Wireless_v1_00.tsv.gz\"\n",
        "#                 \"https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Watches_v1_00.tsv.gz\",\n",
        "#                 \"https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Video_Games_v1_00.tsv.gz\",\n",
        "#                 \"https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Video_DVD_v1_00.tsv.gz\",\n",
        "#                 \"https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Video_v1_00.tsv.gz\",\n",
        "#                 \"https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Toys_v1_00.tsv.gz\",\n",
        "#                 \"https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Tools_v1_00.tsv.gz\",\n",
        "#                 \"https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Sports_v1_00.tsv.gz\",\n",
        "#                 \"https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Software_v1_00.tsv.gz\",\n",
        "#                 \"https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Shoes_v1_00.tsv.gz\",\n",
        "#                 \"https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Pet_Products_v1_00.tsv.gz\",\n",
        "#                 \"https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Personal_Care_Appliances_v1_00.tsv.gz\",\n",
        "#                 \"https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_PC_v1_00.tsv.gz\",\n",
        "#                 \"https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Outdoors_v1_00.tsv.gz\",\n",
        "#                 \"https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Office_Products_v1_00.tsv.gz\",\n",
        "#                 \"https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Musical_Instruments_v1_00.tsv.gz\",\n",
        "#                 \"https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Music_v1_00.tsv.gz\",\n",
        "#                 \"https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Mobile_Electronics_v1_00.tsv.gz\",\n",
        "#                 \"https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Mobile_Apps_v1_00.tsv.gz\",\n",
        "#                 \"https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Major_Appliances_v1_00.tsv.gz\",\n",
        "#                 \"https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Luggage_v1_00.tsv.gz\",\n",
        "#                 \"https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Lawn_and_Garden_v1_00.tsv.gz\",\n",
        "#                 \"https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Kitchen_v1_00.tsv.gz\",\n",
        "#                 \"https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Jewelry_v1_00.tsv.gz\",\n",
        "#                 \"https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Home_Improvement_v1_00.tsv.gz\",\n",
        "#                 \"https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Home_Entertainment_v1_00.tsv.gz\",\n",
        "#                 \"https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Home_v1_00.tsv.gz\",\n",
        "#                 \"https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Health_Personal_Care_v1_00.tsv.gz\",\n",
        "#                 \"https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Grocery_v1_00.tsv.gz\",\n",
        "#                 \"https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Gift_Card_v1_00.tsv.gz\",\n",
        "#                 \"https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Furniture_v1_00.tsv.gz\",\n",
        "#                 \"https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Electronics_v1_00.tsv.gz\",\n",
        "#                 \"https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Digital_Video_Games_v1_00.tsv.gz\",\n",
        "#                 \"https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Digital_Video_Download_v1_00.tsv.gz\",\n",
        "#                 \"https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Digital_Software_v1_00.tsv.gz\",\n",
        "#                 \"https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Digital_Music_Purchase_v1_00.tsv.gz\",\n",
        "#                 \"https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Digital_Ebook_Purchase_v1_01.tsv.gz\",\n",
        "#                 \"https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Digital_Ebook_Purchase_v1_00.tsv.gz\",\n",
        "#                 \"https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Camera_v1_00.tsv.gz\",\n",
        "#                 \"https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Books_v1_02.tsv.gz\",\n",
        "#                 \"https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Books_v1_01.tsv.gz\",\n",
        "#                 \"https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Books_v1_00.tsv.gz\",\n",
        "#                 \"https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Beauty_v1_00.tsv.gz\",\n",
        "#                 \"https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Baby_v1_00.tsv.gz\",\n",
        "#                 \"https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Automotive_v1_00.tsv.gz\",\n",
        "#                 \"https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Apparel_v1_00.tsv.gz\",\n",
        "               ]\n",
        "\n",
        "#download dataset for each category, store in drive\n",
        "for url in review_paths:\n",
        "  print(\"starded downloading \" + url)\n",
        "  category = re.search('us_(.+?)_v1', url).group(1)\n",
        "  file_path = compressed_path + category + \".tsv.gz\"\n",
        "  r = requests.get(url, allow_redirects=True)\n",
        "  open(file_path, 'wb').write(r.content)\n",
        "  print(\"finished downloading \" + url)\n",
        "\n",
        "#unzip datasets  \n",
        "#ungzip(compressed_path, root)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "F5GjkAYq-I0_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Data Collection and Integration\n"
      ]
    },
    {
      "metadata": {
        "id": "8hW9_6hN7NL5",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title Data Filters\n",
        "\n",
        "include_all_categories = False #@param {type:\"boolean\"}\n",
        "correct_bias = True #@param {type:\"boolean\"}\n",
        "pick_category = \"Digital_Ebook_Purchase\"  #@param ['Apparel', 'Automotive', 'Baby', 'Beauty', 'Books', 'Books_v1_00', 'Books_v1_01', 'Camera', 'Digital_Ebook_Purchase', 'Digital_Ebook_Purchase1', 'Digital_Music_Purchase', 'Digital_Software', 'Digital_Video_Download', 'Digital_Video_Games', 'Electronics', 'Furniture', 'Gift_Card', 'Grocery', 'Health_Personal_Care', 'Home', 'Home_Entertainment', 'Home_Improvement', 'Jewelry', 'Kitchen', 'Lawn_and_Garden', 'Luggage', 'Major_Appliances', 'Mobile_Apps', 'Mobile_Electronics', 'Music', 'Musical_Instruments', 'Office_Products', 'Outdoors', 'PC', 'Personal_Care_Appliances', 'Pet_Products', 'Shoes', 'Software', 'Sports', 'Tools', 'Toys', 'Video', 'Video_DVD', 'Video_Games', 'Watches', 'Wireless']\n",
        "entry_limit = 413000  #@param {type: \"slider\", min: 1000, max: 5000000, step:1000}\n",
        "#@markdown ---\n",
        "#@markdown ###Review Filters\n",
        "filter_reviews_older_than = '1995-01-01'  #@param {type: \"date\"}\n",
        "\n",
        "#generate ds paths\n",
        "root = \"drive/Team Drives/DS Workshop/data/\"\n",
        "file_ext = \".tsv\"\n",
        "ds_path = root + pick_category + file_ext\n",
        "\n",
        "file_names = glob.glob1(root, \"*\" + file_ext)\n",
        "all_paths = [root + filename for filename in file_names]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3w6qa9pSqzNP",
        "colab_type": "code",
        "outputId": "76d20d34-23f9-47cf-a54f-1becdd6c5894",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1176
        }
      },
      "cell_type": "code",
      "source": [
        "if include_all_categories:\n",
        "  ds_list = []\n",
        "  for path in all_paths:\n",
        "    print('loading ' + path)\n",
        "    cat_ds = pd.read_csv(path, \n",
        "                        delimiter='\\t',\n",
        "                        warn_bad_lines=True,\n",
        "                        error_bad_lines=False,\n",
        "                        nrows=entry_limit)\n",
        "    ds_list.append(cat_ds)\n",
        "    reviews = pd.concat(ds_list, axis = 0, ignore_index = True)\n",
        "    \n",
        "  ds_list = []\n",
        "else:\n",
        "  reviews = pd.read_csv(ds_path, \n",
        "                        delimiter='\\t',\n",
        "                        warn_bad_lines=True,\n",
        "                        error_bad_lines=False,\n",
        "                        nrows=entry_limit)\n",
        "\n",
        "# reviews = reviews.apply(lambda row: isLaterThan(row['review_date'], filter_reviews_older_than))\n",
        "\n",
        "reviews = reviews[reviews['review_date'] > filter_reviews_older_than]\n",
        "\n",
        "# reviews['unhelpful_votes'] = reviews['total_votes'] - reviews['helpful_votes']\n",
        "\n",
        "# add helpful column. A review is considered helpful if it was upvoted at least once\n",
        "# reviews['helpful'] = reviews['helpful_votes'] > 0\n",
        "\n",
        "# thumbs = pd.DataFrame(reviews.groupby('customer_id').mean())\n",
        "# thumbs = thumbs[['helpful_votes','unhelpful_votes']]\n",
        "# thumbs.columns = ['helpful_votes_avg','helpful_votes_avg']\n",
        "commentCount=pd.DataFrame(reviews.groupby('customer_id').size())\n",
        "commentCount.columns=['commentCount']\n",
        "reviews = reviews.join(commentCount, on='customer_id')\n",
        "\n",
        "print('Description: \\n')\n",
        "print(reviews.describe())\n",
        "print('Data types: \\n')\n",
        "print(reviews.dtypes)\n",
        "reviews.head()\n",
        "\n",
        "# reviews.count()"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Description: \n",
            "\n",
            "        customer_id  product_parent    star_rating  helpful_votes  \\\n",
            "count  4.129850e+05    4.129850e+05  412985.000000  412985.000000   \n",
            "mean   2.867195e+07    4.946859e+08       4.304604       0.998329   \n",
            "std    1.481085e+07    2.877486e+08       1.047085       4.573339   \n",
            "min    1.034700e+04    3.474000e+03       1.000000       0.000000   \n",
            "25%    1.507327e+07    2.416769e+08       4.000000       0.000000   \n",
            "50%    2.755814e+07    4.920764e+08       5.000000       0.000000   \n",
            "75%    4.283653e+07    7.456701e+08       5.000000       1.000000   \n",
            "max    5.309646e+07    9.999985e+08       5.000000     648.000000   \n",
            "\n",
            "         total_votes   commentCount  \n",
            "count  412985.000000  412985.000000  \n",
            "mean        1.390787       4.960386  \n",
            "std         5.533420      11.363703  \n",
            "min         0.000000       1.000000  \n",
            "25%         0.000000       1.000000  \n",
            "50%         0.000000       2.000000  \n",
            "75%         1.000000       4.000000  \n",
            "max       742.000000     241.000000  \n",
            "Data types: \n",
            "\n",
            "marketplace           object\n",
            "customer_id            int64\n",
            "review_id             object\n",
            "product_id            object\n",
            "product_parent         int64\n",
            "product_title         object\n",
            "product_category      object\n",
            "star_rating          float64\n",
            "helpful_votes        float64\n",
            "total_votes          float64\n",
            "vine                  object\n",
            "verified_purchase     object\n",
            "review_headline       object\n",
            "review_body           object\n",
            "review_date           object\n",
            "commentCount           int64\n",
            "dtype: object\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>marketplace</th>\n",
              "      <th>customer_id</th>\n",
              "      <th>review_id</th>\n",
              "      <th>product_id</th>\n",
              "      <th>product_parent</th>\n",
              "      <th>product_title</th>\n",
              "      <th>product_category</th>\n",
              "      <th>star_rating</th>\n",
              "      <th>helpful_votes</th>\n",
              "      <th>total_votes</th>\n",
              "      <th>vine</th>\n",
              "      <th>verified_purchase</th>\n",
              "      <th>review_headline</th>\n",
              "      <th>review_body</th>\n",
              "      <th>review_date</th>\n",
              "      <th>commentCount</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>US</td>\n",
              "      <td>33605939</td>\n",
              "      <td>RGYFDX8QXKEIR</td>\n",
              "      <td>B007KO2MLO</td>\n",
              "      <td>328837464</td>\n",
              "      <td>Big Maria</td>\n",
              "      <td>Digital_Ebook_Purchase</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>Quirky</td>\n",
              "      <td>Elmore Leonard meets the cast of Sierra Madre....</td>\n",
              "      <td>2013-09-09</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>US</td>\n",
              "      <td>34058393</td>\n",
              "      <td>R13CBGTMNV9R8Z</td>\n",
              "      <td>B005FLODDE</td>\n",
              "      <td>764276359</td>\n",
              "      <td>The Woman Who Wasn't There: The True Story of ...</td>\n",
              "      <td>Digital_Ebook_Purchase</td>\n",
              "      <td>4.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>N</td>\n",
              "      <td>Y</td>\n",
              "      <td>The Woman Who Wasn't There</td>\n",
              "      <td>This book was very interesting. It is a true s...</td>\n",
              "      <td>2013-09-09</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>US</td>\n",
              "      <td>39601147</td>\n",
              "      <td>R7DRFHC0F71O0</td>\n",
              "      <td>B00EA3L35O</td>\n",
              "      <td>535606445</td>\n",
              "      <td>Mary had A Sleepy Sheep</td>\n",
              "      <td>Digital_Ebook_Purchase</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>This Sleepy Sheep rocks!</td>\n",
              "      <td>I had the opportunity to review Mary had a Sle...</td>\n",
              "      <td>2013-09-09</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>US</td>\n",
              "      <td>17351407</td>\n",
              "      <td>R27LUKEXU3KBXQ</td>\n",
              "      <td>B00BL3JV50</td>\n",
              "      <td>240053004</td>\n",
              "      <td>Starstruck</td>\n",
              "      <td>Digital_Ebook_Purchase</td>\n",
              "      <td>5.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>N</td>\n",
              "      <td>Y</td>\n",
              "      <td>Steamy and suspenseful!!!!!</td>\n",
              "      <td>What a great read!  I really couldn't put this...</td>\n",
              "      <td>2013-09-09</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>US</td>\n",
              "      <td>10463387</td>\n",
              "      <td>R1VXTPUYMNU687</td>\n",
              "      <td>B00CXU7U80</td>\n",
              "      <td>931529805</td>\n",
              "      <td>The Complete Conan Saga</td>\n",
              "      <td>Digital_Ebook_Purchase</td>\n",
              "      <td>5.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>Barbarians</td>\n",
              "      <td>Barbarians need love too !  Short stories work...</td>\n",
              "      <td>2013-09-09</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  marketplace  customer_id       review_id  product_id  product_parent  \\\n",
              "0          US     33605939   RGYFDX8QXKEIR  B007KO2MLO       328837464   \n",
              "1          US     34058393  R13CBGTMNV9R8Z  B005FLODDE       764276359   \n",
              "2          US     39601147   R7DRFHC0F71O0  B00EA3L35O       535606445   \n",
              "3          US     17351407  R27LUKEXU3KBXQ  B00BL3JV50       240053004   \n",
              "4          US     10463387  R1VXTPUYMNU687  B00CXU7U80       931529805   \n",
              "\n",
              "                                       product_title        product_category  \\\n",
              "0                                          Big Maria  Digital_Ebook_Purchase   \n",
              "1  The Woman Who Wasn't There: The True Story of ...  Digital_Ebook_Purchase   \n",
              "2                            Mary had A Sleepy Sheep  Digital_Ebook_Purchase   \n",
              "3                                         Starstruck  Digital_Ebook_Purchase   \n",
              "4                            The Complete Conan Saga  Digital_Ebook_Purchase   \n",
              "\n",
              "   star_rating  helpful_votes  total_votes vine verified_purchase  \\\n",
              "0          4.0            0.0          0.0    N                 N   \n",
              "1          4.0            1.0          2.0    N                 Y   \n",
              "2          5.0            0.0          0.0    N                 N   \n",
              "3          5.0            1.0          1.0    N                 Y   \n",
              "4          5.0            1.0          2.0    N                 N   \n",
              "\n",
              "               review_headline  \\\n",
              "0                       Quirky   \n",
              "1   The Woman Who Wasn't There   \n",
              "2     This Sleepy Sheep rocks!   \n",
              "3  Steamy and suspenseful!!!!!   \n",
              "4                   Barbarians   \n",
              "\n",
              "                                         review_body review_date  commentCount  \n",
              "0  Elmore Leonard meets the cast of Sierra Madre....  2013-09-09             1  \n",
              "1  This book was very interesting. It is a true s...  2013-09-09             2  \n",
              "2  I had the opportunity to review Mary had a Sle...  2013-09-09             2  \n",
              "3  What a great read!  I really couldn't put this...  2013-09-09             3  \n",
              "4  Barbarians need love too !  Short stories work...  2013-09-09             8  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "metadata": {
        "id": "k3mxZGWOVymn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UHw7wE5xKkqS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Data Preperation and Cleaning"
      ]
    },
    {
      "metadata": {
        "id": "yzVcQ-BtKrhm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Data Visualization and Analysis"
      ]
    },
    {
      "metadata": {
        "id": "yMddV3Lqutdp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# #wordcloud\n",
        "# text = reviews['review_body']\n",
        "# wordcloud = WordCloud(\n",
        "#     width = 1500,\n",
        "#     height = 1000,\n",
        "#     background_color = 'white',\n",
        "#     ).generate(str(text))\n",
        "# fig = plt.figure(\n",
        "#     figsize = (18, 12),\n",
        "#     facecolor = 'w',\n",
        "#     edgecolor = 'w')\n",
        "# plt.imshow(wordcloud, interpolation = 'bilinear')\n",
        "# plt.axis('off')\n",
        "# plt.tight_layout(pad=0)\n",
        "# plt.show()\n",
        "\n",
        "# wordcloud = []"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JF_tMXpnK-9s",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Feature Selection and Engineering"
      ]
    },
    {
      "metadata": {
        "id": "NDQJyenJPy8C",
        "colab_type": "code",
        "outputId": "2b5c884c-4c22-469f-f461-733d40f2f399",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        }
      },
      "cell_type": "code",
      "source": [
        "def add_meta_features(dataset):\n",
        "  dataset['helpful'] = ApplyHelpfulnessVector(dataset) \n",
        "  \n",
        "def ApplyHelpfulnessVector(reviews):\n",
        "  return reviews.apply(lambda row: isHelpful(row['total_votes'], row['helpful_votes']), axis=1)\n",
        "  \n",
        "def isHelpful(total_votes, helpful_votes):\n",
        "  if (total_votes >=4 and helpful_votes > total_votes - helpful_votes):\n",
        "      return 1\n",
        "  elif (total_votes < 4):\n",
        "      return 0\n",
        "  else:\n",
        "      return -1\n",
        "\n",
        "add_meta_features(reviews)\n",
        "\n",
        "# reviews[reviews['helpful'] > 0].head(10) # debug\n",
        "reviews.describe() #debug"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>customer_id</th>\n",
              "      <th>product_parent</th>\n",
              "      <th>star_rating</th>\n",
              "      <th>helpful_votes</th>\n",
              "      <th>total_votes</th>\n",
              "      <th>commentCount</th>\n",
              "      <th>helpful</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>4.129850e+05</td>\n",
              "      <td>4.129850e+05</td>\n",
              "      <td>412985.000000</td>\n",
              "      <td>412985.000000</td>\n",
              "      <td>412985.000000</td>\n",
              "      <td>412985.000000</td>\n",
              "      <td>412985.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>2.867195e+07</td>\n",
              "      <td>4.946859e+08</td>\n",
              "      <td>4.304604</td>\n",
              "      <td>0.998329</td>\n",
              "      <td>1.390787</td>\n",
              "      <td>4.960386</td>\n",
              "      <td>0.041864</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>1.481085e+07</td>\n",
              "      <td>2.877486e+08</td>\n",
              "      <td>1.047085</td>\n",
              "      <td>4.573339</td>\n",
              "      <td>5.533420</td>\n",
              "      <td>11.363703</td>\n",
              "      <td>0.289311</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>1.034700e+04</td>\n",
              "      <td>3.474000e+03</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>1.507327e+07</td>\n",
              "      <td>2.416769e+08</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>2.755814e+07</td>\n",
              "      <td>4.920764e+08</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>4.283653e+07</td>\n",
              "      <td>7.456701e+08</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>5.309646e+07</td>\n",
              "      <td>9.999985e+08</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>648.000000</td>\n",
              "      <td>742.000000</td>\n",
              "      <td>241.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        customer_id  product_parent    star_rating  helpful_votes  \\\n",
              "count  4.129850e+05    4.129850e+05  412985.000000  412985.000000   \n",
              "mean   2.867195e+07    4.946859e+08       4.304604       0.998329   \n",
              "std    1.481085e+07    2.877486e+08       1.047085       4.573339   \n",
              "min    1.034700e+04    3.474000e+03       1.000000       0.000000   \n",
              "25%    1.507327e+07    2.416769e+08       4.000000       0.000000   \n",
              "50%    2.755814e+07    4.920764e+08       5.000000       0.000000   \n",
              "75%    4.283653e+07    7.456701e+08       5.000000       1.000000   \n",
              "max    5.309646e+07    9.999985e+08       5.000000     648.000000   \n",
              "\n",
              "         total_votes   commentCount        helpful  \n",
              "count  412985.000000  412985.000000  412985.000000  \n",
              "mean        1.390787       4.960386       0.041864  \n",
              "std         5.533420      11.363703       0.289311  \n",
              "min         0.000000       1.000000      -1.000000  \n",
              "25%         0.000000       1.000000       0.000000  \n",
              "50%         0.000000       2.000000       0.000000  \n",
              "75%         1.000000       4.000000       0.000000  \n",
              "max       742.000000     241.000000       1.000000  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "metadata": {
        "id": "xwgB43SZg8lx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Bias Correction"
      ]
    },
    {
      "metadata": {
        "id": "wWVtdZOEhQVv",
        "colab_type": "code",
        "outputId": "54a770a9-9cc2-444d-c86e-f57f01fcafa4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "cell_type": "code",
      "source": [
        "print(reviews.count()[0])\n",
        "\n",
        "df1 = reviews[reviews['helpful'] == -1]\n",
        "df2 = reviews[reviews['helpful'] == 1]\n",
        "if correct_bias:\n",
        "  df2 = reviews[reviews['helpful'] == 1].sample(n=df1.count()[0])\n",
        "reviews = pd.concat([df1,df2])\n",
        "\n",
        "print(reviews[reviews['helpful'] == 1].count()[0])\n",
        "print(reviews[reviews['helpful'] == 0].count()[0])\n",
        "print(reviews[reviews['helpful'] == -1].count()[0])"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "412985\n",
            "9001\n",
            "0\n",
            "9001\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "P7UrpB3ZK2cA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Model Training"
      ]
    },
    {
      "metadata": {
        "id": "_gj_lFiONKmx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# # CNN for the IMDB problem\n",
        "# import numpy as np\n",
        "# from keras.preprocessing.text import Tokenizer\n",
        "# from keras.models import Sequential\n",
        "# from keras.layers import Dense\n",
        "# from keras.layers import LSTM, Convolution1D, Flatten, Dropout\n",
        "# from keras.layers.convolutional import Conv1D\n",
        "# from keras.layers.convolutional import MaxPooling1D\n",
        "# from keras.layers.embeddings import Embedding\n",
        "# from keras.preprocessing.sequence import pad_sequences\n",
        "# import re\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# from sklearn.preprocessing import LabelEncoder\n",
        "# import pickle\n",
        "# from keras.utils import np_utils\n",
        "# import pandas as pd\n",
        "# from sklearn.model_selection import cross_val_score\n",
        "# from sklearn.model_selection import KFold\n",
        "# from keras.wrappers.scikit_learn import KerasClassifier\n",
        "\n",
        "\n",
        "# data = reviews\n",
        "# data = data.loc[:, ['review_body','helpful']]\n",
        "\n",
        "\n",
        "# #Convert reviews text to lowercase\n",
        "# data['text'] = data['review_body'].apply(lambda x: str(x).lower())\n",
        "# #Remove all charactares appart from a-zA-z0-9 (for instance: punctuation)\n",
        "# data['text'] = data['text'].apply((lambda x: re.sub('[^a-zA-z0-9\\s]', '', x)))\n",
        "\n",
        "# #overallDistribution = data.overall.value_counts().sort_index()\n",
        "# #print(overallDistribution)\n",
        "\n",
        "# #Vocabulary size - Most 2000 common words\n",
        "# #Convert input text to integer sequences\n",
        "# max_features = 5000\n",
        "# tokenizer = Tokenizer(num_words=max_features, split=' ')\n",
        "# tokenizer.fit_on_texts(data['text'].values)\n",
        "# X = tokenizer.texts_to_sequences(data['text'].values)\n",
        "# #cap the maximum review length at 500 words,\n",
        "# #truncating reviews longer than that and\n",
        "# # padding reviews shorter than that with 0 values.\n",
        "# max_review_length = 500\n",
        "# X = pad_sequences(X,  maxlen=max_review_length)\n",
        "\n",
        "# # Using embedding from Keras\n",
        "# embedding_vector_length = 32\n",
        "# def baseline_model():\n",
        "#     # create the model - Machine learning mastery\n",
        "#     model = Sequential()\n",
        "#     model.add(Embedding(max_features, embedding_vector_length, input_length=max_review_length))\n",
        "#     model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
        "#     model.add(MaxPooling1D(pool_size=2))\n",
        "#     model.add(Flatten())\n",
        "#     # model.add(Dropout(0.1))\n",
        "#     model.add(Dense(250, activation='relu'))\n",
        "#     # model.add(Dense(1, activation='sigmoid'))\n",
        "#     model.add(Dense(1, activation='sigmoid'))\n",
        "#     model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "#     print(model.summary())\n",
        "#     return model\n",
        "\n",
        "# Y = data['helpful']\n",
        "# # Fit the model\n",
        "# epochs_num = 2\n",
        "# batchSize = 128\n",
        "\n",
        "# model = KerasClassifier(build_fn=baseline_model, epochs=epochs_num, batch_size=batchSize, verbose=0)\n",
        "\n",
        "# nsplits = 3\n",
        "# kfold = KFold(n_splits=nsplits, shuffle=True, random_state=1)\n",
        "# results = cross_val_score(model, X, Y, cv=kfold)\n",
        "# print(\"Baseline: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MUnJm0wjGMQ_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Non DL Models"
      ]
    },
    {
      "metadata": {
        "id": "hTEb20xcAWWH",
        "colab_type": "code",
        "outputId": "03b9a06c-98d1-418e-d819-1d05738d6331",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        }
      },
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "import numpy as np\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.datasets.twenty_newsgroups import strip_newsgroup_footer\n",
        "from sklearn.datasets.twenty_newsgroups import strip_newsgroup_quoting\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.feature_extraction import DictVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.feature_selection import SelectKBest, chi2\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.pipeline import FeatureUnion\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.svm import SVC\n",
        "import pickle\n",
        "from sklearn import metrics\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "from nltk.stem import SnowballStemmer\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import KFold\n",
        "import nltk\n",
        "import string\n",
        "from sklearn.model_selection import cross_validate\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "data = reviews\n",
        "data = data.loc[:, ['review_body','helpful']]\n",
        "\n",
        "X = data['review_body'].astype('U')\n",
        "Y = data['helpful']\n",
        "\n",
        "print(X.size)\n",
        "print(Y.size)\n",
        "print(stopwords.words('english'))\n",
        "\n",
        "def stemTokenizer(sentence):\n",
        "    ret = RegexpTokenizer('[a-zA-Z0-9\\']+')\n",
        "    sw = set(stopwords.words('english'))\n",
        "    tokens= ret.tokenize(sentence)\n",
        "    ess = SnowballStemmer('english', ignore_stopwords=True)\n",
        "    return [ess.stem(t) for t in tokens if t not in sw]\n",
        "\n",
        "def lemmaTokenizer(sentence):\n",
        "    ret = RegexpTokenizer('[a-zA-Z0-9\\']+')\n",
        "    sw = set(stopwords.words('english'))\n",
        "    tokens= ret.tokenize(sentence)\n",
        "    wnl = WordNetLemmatizer()\n",
        "\n",
        "    #return [wnl.lemmatize(t) for t in tokens if t not in sw]\n",
        "    return [wnl.lemmatize(t) for t in tokens]\n",
        "\n",
        "  \n",
        "class TextStats(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"Extract features from each document for DictVectorizer\"\"\"\n",
        "\n",
        "    def fit(self, x, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, posts):\n",
        "        return [{'length': len(text)}  for text in posts]\n",
        "\n",
        "class Debug(BaseEstimator, TransformerMixin):\n",
        "    def fit(self, x, y=None):\n",
        "        return self\n",
        "    \n",
        "    def transform(self, posts):\n",
        "        print(posts)\n",
        "        return self\n",
        "\n",
        "pipeline = Pipeline([\n",
        "    # Use FeatureUnion to combine the features from subject and body\n",
        "    ('union', FeatureUnion(\n",
        "        transformer_list=[\n",
        "\n",
        "            # Pipeline for standard bag-of-words model for body\n",
        "            ('body_bow', Pipeline([\n",
        "                \n",
        "                ('vect', CountVectorizer(tokenizer = lemmaTokenizer,\n",
        "                                         \n",
        "                                ngram_range=(1, 2),\n",
        "                                binary = True, min_df=2, max_df=0.8)),\n",
        "                        ('tfidf', TfidfTransformer()),\n",
        "                \n",
        "            ])),\n",
        "            \n",
        "            # Pipeline for pulling ad hoc features from post's body\n",
        "            ('body_stats', Pipeline([\n",
        "                ('stats', TextStats()),  # returns a list of dicts\n",
        "                ('vect', DictVectorizer()),  # list of dicts -> feature matrix\n",
        "            ])),\n",
        "        ],\n",
        "        # weight components in FeatureUnion\n",
        "        transformer_weights={\n",
        "            'body_bow': 0.85,\n",
        "            'body_stats': 0.15,\n",
        "        },\n",
        "    )),\n",
        "    ('chi2', SelectKBest(chi2, k=500)), #5000\n",
        "    # Use a SVC classifier on the combined features\n",
        "    #('clf', SGDClassifier( penalty='elasticnet')),\n",
        "     ('clf', MultinomialNB(fit_prior=False)),\n",
        "    # ('clf', LogisticRegression()),\n",
        "    #('clf', RandomForestClassifier(n_estimators=100))\n",
        "])\n",
        "\n",
        "#pipeline.fit(X, Y)\n",
        "\n",
        "from sklearn.metrics.scorer import make_scorer\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "def tp(y_true, y_pred): return confusion_matrix(y_true, y_pred)[0, 0]\n",
        "def tn(y_true, y_pred): return confusion_matrix(y_true, y_pred)[1, 1]\n",
        "def fp(y_true, y_pred): return confusion_matrix(y_true, y_pred)[1, 0]\n",
        "def fn(y_true, y_pred): return confusion_matrix(y_true, y_pred)[0, 1]\n",
        "scoring = {'tp' : make_scorer(tp), 'tn' : make_scorer(tn),\n",
        "           'fp' : make_scorer(fp), 'fn' : make_scorer(fn)}\n",
        "\n",
        "scoring = {'acc':'accuracy',\n",
        "           'f1':'f1', \n",
        "           'prec':'precision', \n",
        "           'rec':'recall',\n",
        "           'roc': 'roc_auc',\n",
        "           'tp' : make_scorer(tp),\n",
        "           'tn' : make_scorer(tn),\n",
        "           'fp' : make_scorer(fp),\n",
        "           'fn' : make_scorer(fn)\n",
        "          }\n",
        "\n",
        "nsplits = 3\n",
        "kfold = KFold(n_splits=nsplits, shuffle=True, random_state=1)\n",
        "results = cross_validate(pipeline, X, Y, cv=kfold, scoring=scoring)\n",
        "print(results)\n",
        "#print(\"Baseline: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "18002\n",
            "18002\n",
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "f41mx37QoELi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Flow Understanding\n"
      ]
    },
    {
      "metadata": {
        "id": "cfxW6g9enLMp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "cnt_vectorizer = CountVectorizer(tokenizer = lemmaTokenizer,\n",
        "                                         \n",
        "                                ngram_range=(1, 2),\n",
        "                                binary = True, min_df=2, max_df=0.8)\n",
        "\n",
        "X1 = cnt_vectorizer.fit_transform(X)\n",
        "feats = np.array(cnt_vectorizer.get_feature_names())\n",
        "print(X1.toarray())  \n",
        "print(X1.shape)\n",
        "\n",
        "tfidf_transformer = TfidfTransformer()\n",
        "X_train_tfidf = tfidf_transformer.fit_transform(X1)\n",
        "\n",
        "kbest = SelectKBest(chi2, k=5)\n",
        "kbest.fit(X_train_tfidf, Y)\n",
        "selected = kbest.get_support()\n",
        "print(feats[np.array(selected)])\n",
        "\n",
        "# print(X_train_tfidf)\n",
        "# print(X_train_tfidf.shape)\n",
        "# print(X_train_tfidf[0])\n",
        "\n",
        "# dict={}\n",
        "# for doc in X_train_tfidf:\n",
        "#   x=3\n",
        "# dict[44]=33\n",
        "# # print(dict)\n",
        "# print(X_train_tfidf[0,:][0,:])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OJBNRXGVveaX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## debug"
      ]
    },
    {
      "metadata": {
        "id": "9tN89e5HsSKH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "conf_mat = [[sum(results['test_tn']), sum(results['test_fp'])],\n",
        "            [sum(results['test_fn']), sum(results['test_tp'])]]\n",
        "conf_mat = np.array(conf_mat)\n",
        "\n",
        "from sklearn.model_selection import cross_val_predict\n",
        "y_pred = cross_val_predict(pipeline, X, Y, cv=kfold)\n",
        "# cnf_matrix = confusion_matrix(Y, y_pred)\n",
        "\n",
        "ones = Y[y_pred == 1]\n",
        "minus = Y[y_pred == -1]\n",
        "\n",
        "print(ones[ones == 1].size) # tp\n",
        "# print(ones[ones == -1].size) # fp\n",
        "\n",
        "print(minus[minus == -1].size) # tn\n",
        "# print(minus[minus == 1].size) # fn\n",
        "\n",
        "\n",
        "print(conf_mat)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yArMykyMsei1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "v6E3imajLJqb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Model Evaluation"
      ]
    },
    {
      "metadata": {
        "id": "USdO5udhsd2Z",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Results"
      ]
    },
    {
      "metadata": {
        "id": "pUta4_QFscLX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "scUOGTqHsXhe",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Plot Confusion Matrix"
      ]
    },
    {
      "metadata": {
        "id": "Dr96xCZu33ji",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import itertools\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def plot_confusion_matrix(cm, classes,\n",
        "                          normalize=False,\n",
        "                          title='Confusion matrix',\n",
        "                          cmap=plt.cm.Blues):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    \"\"\"\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        print(\"Normalized confusion matrix\")\n",
        "    else:\n",
        "        print('Confusion matrix, without normalization')\n",
        "\n",
        "    print(cm)\n",
        "\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    fmt = '.2f' if normalize else 'd'\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, format(cm[i, j], fmt),\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "    plt.tight_layout()\n",
        "\n",
        "\n",
        "\n",
        "np.set_printoptions(precision=2)\n",
        "\n",
        "# Plot non-normalized confusion matrix\n",
        "plt.figure()\n",
        "plot_confusion_matrix(conf_mat, classes=['helpful','unhelpful'],\n",
        "                      title='Confusion matrix, without normalization')\n",
        "\n",
        "# Plot normalized confusion matrix\n",
        "plt.figure()\n",
        "plot_confusion_matrix(conf_mat, classes=['helpful','unhelpful'], normalize=True,\n",
        "                      title='Normalized confusion matrix')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wUzCqyseyVdj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}